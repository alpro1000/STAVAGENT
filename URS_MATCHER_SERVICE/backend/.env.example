# ============================================================================
# LLM CONFIGURATION - MULTI-PROVIDER SUPPORT WITH FALLBACK
# ============================================================================
# You can specify all three providers and the system will:
# 1. Use PRIMARY provider (LLM_PROVIDER)
# 2. Automatically fallback to alternates if primary fails
# 3. Switch providers based on cost/quality/speed needs
#
# RECOMMENDED SETUP FOR RENDER:
# Set all three provider keys + choose primary in LLM_PROVIDER

# Primary provider to use (fallback chain depends on this)
# Options: gemini (cheapest, recommended), claude (best quality), openai (balanced)
LLM_PROVIDER=gemini

# IMPORTANT: Timeout for LLM API calls
# 90 seconds is recommended because:
# - Render services may need cold start time (5-10 seconds)
# - Claude complex analysis can take 30-60 seconds
# - Fallback chain needs time to try multiple providers
# Do NOT set this to less than 60000 (60 seconds) in production!
LLM_TIMEOUT_MS=90000

# ============================================================================
# CLAUDE (ANTHROPIC) - Optional but recommended
# ============================================================================
# Get key: https://console.anthropic.com/keys
# Best for: Complex reasoning, highest quality output
# Cost: $0.30/min, Speed: 8/10, Quality: 10/10
# Fallback order if primary: 1st fallback for Claude as primary

ANTHROPIC_API_KEY=sk-ant-YOUR_CLAUDE_API_KEY_HERE
CLAUDE_MODEL=claude-sonnet-4-5-20250929

# ============================================================================
# GOOGLE GEMINI - Optional (RECOMMENDED AS PRIMARY - CHEAPEST)
# ============================================================================
# Get key: https://aistudio.google.com/app/apikeys
# Best for: Cost-effective, fast processing
# Cost: $0.001/min (300x cheaper than OpenAI!), Speed: 10/10, Quality: 8/10
# Fallback order if primary: 1st fallback for Gemini as primary

GOOGLE_API_KEY=YOUR_GOOGLE_GEMINI_API_KEY_HERE
GEMINI_MODEL=gemini-2.0-flash

# ============================================================================
# OPENAI - Optional (fallback provider)
# ============================================================================
# Get key: https://platform.openai.com/api/keys
# Best for: General purpose, well-tested, popular models
# Cost: $1.00/min, Speed: 7/10, Quality: 9/10
# Fallback order if primary: 3rd fallback (last resort)

OPENAI_API_KEY=sk-proj-YOUR_OPENAI_API_KEY_HERE
OPENAI_MODEL=gpt-4-turbo

# ============================================================================
# PERPLEXITY CONFIGURATION (optional)
# ============================================================================
# Get key: https://www.perplexity.ai/api
# Used for: Real-time knowledge base search
# GitHub Secret Name: PPLX_API_KEY
# Render Env Var: PPLX_API_KEY

PPLX_API_KEY=pplx-YOUR_PERPLEXITY_API_KEY_HERE
PPLX_MODEL=sonar
PPLX_TIMEOUT_MS=60000

# ============================================================================
# NODE ENVIRONMENT
# ============================================================================
NODE_ENV=development
PORT=3001

# ============================================================================
# REDIS CACHE (OPTIONAL - FOR HORIZONTAL SCALING)
# ============================================================================
# Redis is used for:
# 1. LLM provider failure cache (cross-instance coordination)
# 2. Document parsing cache
# 3. LLM response cache
#
# For single-instance deployment: Leave empty (uses in-memory cache)
# For multi-instance/scaled deployment: Configure Redis
#
# Local development (Docker): redis://redis:6379
# Render Redis addon: Will be set automatically via REDIS_URL
# External Redis: redis://user:password@host:port/db
REDIS_URL=

# ============================================================================
# NOTES FOR DEPLOYMENT
# ============================================================================
# For local development:
#   1. Copy this file: cp .env.example .env
#   2. Replace YOUR_*_HERE with actual keys
#   3. .env is in .gitignore (won't be committed)
#
# For GitHub CI/CD:
#   1. Add secrets via GitHub Settings → Secrets and variables → Actions
#   2. Use same names as GitHub Secret Name above
#
# For Render production:
#   1. Add environment variables via Render Dashboard
#   2. Use same names as Render Env Var above
