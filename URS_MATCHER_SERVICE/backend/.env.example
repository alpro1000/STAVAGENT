# ============================================================================
# LLM CONFIGURATION - MULTI-PROVIDER SUPPORT WITH FALLBACK
# ============================================================================
# Multi-provider fallback strategy:
# 1. Use PRIMARY provider (LLM_PROVIDER)
# 2. Automatically fallback to alternates if primary fails
# 3. Final fallback: Fuzzy matching (no LLM needed)
#
# RECOMMENDED SETUP FOR MAXIMUM RELIABILITY:
# Set all three provider keys + choose primary in LLM_PROVIDER

# Primary provider to use (fallback chain depends on this)
# Options: gemini (cheapest), openai (balanced), claude (best quality)
LLM_PROVIDER=gemini

# IMPORTANT: Timeout for LLM API calls (applies to ALL providers)
# 90 seconds recommended:
# - Render cold start: 5-10 seconds
# - Complex analysis: 30-60 seconds
# - Block analysis uses: timeoutMs * 2 = 180 seconds total
# Lower values = premature timeouts, Higher values = slower failures
LLM_TIMEOUT_MS=90000

# ============================================================================
# GOOGLE GEMINI - RECOMMENDED AS PRIMARY (Cheapest & Fastest)
# ============================================================================
# Get key: https://aistudio.google.com/app/apikey
# Best for: Fast responses, lowest cost, good quality
# Cost: $0.075/1M tokens (input), $0.30/1M (output)
# Speed: 10/10, Quality: 8/10
# Fallback order if primary: Gemini → Claude → OpenAI → Fuzzy matching
#
# GitHub Secret Name:  GOOGLE_API_KEY
# Render Env Var:      GOOGLE_API_KEY
GOOGLE_API_KEY=YOUR_GOOGLE_GEMINI_API_KEY_HERE
GEMINI_MODEL=gemini-2.0-flash

# ============================================================================
# OPENAI - BALANCED (Recommended for fallback)
# ============================================================================
# Get key: https://platform.openai.com/api-keys
# Best for: Balanced cost/quality, stable API, good fallback
# Cost: $0.15/1M tokens (input), $0.60/1M (output) - gpt-4o-mini
# Speed: 9/10, Quality: 8/10
# Fallback order if primary: OpenAI → Claude → Gemini → Fuzzy matching
#
# IMPORTANT: Use gpt-4o-mini (66x cheaper than gpt-4-turbo!)
#
# GitHub Secret Name:  OPENAI_API_KEY
# Render Env Var:      OPENAI_API_KEY
OPENAI_API_KEY=sk-proj-YOUR_OPENAI_API_KEY_HERE
OPENAI_MODEL=gpt-4o-mini

# ============================================================================
# CLAUDE (ANTHROPIC) - HIGHEST QUALITY (Expensive but best)
# ============================================================================
# Get key: https://console.anthropic.com/keys
# Best for: Complex reasoning, highest quality output, detailed analysis
# Cost: $3.00/1M tokens (input), $15.00/1M (output)
# Speed: 7/10, Quality: 10/10
# Fallback order if primary: Claude → Gemini → OpenAI → Fuzzy matching
#
# GitHub Secret Name:  ANTHROPIC_API_KEY
# Render Env Var:      ANTHROPIC_API_KEY
ANTHROPIC_API_KEY=sk-ant-YOUR_CLAUDE_API_KEY_HERE
CLAUDE_MODEL=claude-sonnet-4-5-20250929

# ============================================================================
# PERPLEXITY CONFIGURATION (optional)
# ============================================================================
# Get key: https://www.perplexity.ai/api
# Used for: Real-time knowledge base search
# GitHub Secret Name: PPLX_API_KEY
# Render Env Var: PPLX_API_KEY

PPLX_API_KEY=pplx-YOUR_PERPLEXITY_API_KEY_HERE
PPLX_MODEL=sonar
PPLX_TIMEOUT_MS=60000

# ============================================================================
# NODE ENVIRONMENT
# ============================================================================
NODE_ENV=development
PORT=3001

# ============================================================================
# REDIS CACHE (OPTIONAL - FOR HORIZONTAL SCALING)
# ============================================================================
# Redis is used for:
# 1. LLM provider failure cache (cross-instance coordination)
# 2. Document parsing cache
# 3. LLM response cache
#
# For single-instance deployment: Leave empty (uses in-memory cache)
# For multi-instance/scaled deployment: Configure Redis
#
# Local development (Docker): redis://redis:6379
# Render Redis addon: Will be set automatically via REDIS_URL
# External Redis: redis://user:password@host:port/db
REDIS_URL=

# ============================================================================
# STAVAGENT MULTI-ROLE API (OPTIONAL - PHASE 3 INTEGRATION)
# ============================================================================
# Multi-Role API provides 6 specialist AI roles via concrete-agent CORE:
# - Document Validator, Structural Engineer, Concrete Specialist
# - Cost Estimator, Standards Checker, Project Manager
#
# Production URL: https://concrete-agent.onrender.com
# Local development: http://localhost:8000
#
# GitHub Secret Name: STAVAGENT_API_URL
# Render Env Var: STAVAGENT_API_URL
STAVAGENT_API_URL=https://concrete-agent.onrender.com

# Multi-Role API request timeout (default: 90s for Render cold start + processing)
MULTI_ROLE_TIMEOUT_MS=90000

# Multi-Role health check timeout (default: 30s for Render cold start)
MULTI_ROLE_HEALTH_TIMEOUT_MS=30000

# ============================================================================
# NOTES FOR DEPLOYMENT
# ============================================================================
# For local development:
#   1. Copy this file: cp .env.example .env
#   2. Replace YOUR_*_HERE with actual keys
#   3. .env is in .gitignore (won't be committed)
#
# For GitHub CI/CD:
#   1. Add secrets via GitHub Settings → Secrets and variables → Actions
#   2. Use same names as GitHub Secret Name above
#
# For Render production:
#   1. Add environment variables via Render Dashboard
#   2. Use same names as Render Env Var above
